import streamlit as st
import requests
import pandas as pd
from lxml import html
import re
from urllib.parse import quote

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì „ìë„ì„œê´€ í†µí•©ê²€ìƒ‰", page_icon="ğŸ“š", layout="centered")

# 2. ë°ì´í„° ì†Œì…œ ë° API ì„¤ì •
SEOUL_API_KEY = st.secrets.get("seoul_api_key")
SEOCHO_CSV_URL = "https://www.data.go.kr/cmm/cmm/fileDownload.do?atchFileId=FILE_000000003242287&fileDetailSn=1&dataNm=%EC%84%9C%EC%9A%B8%ED%8A%B9%EB%B3%84%EC%8B%9C%20%EC%84%9C%EC%B4%88%EA%B5%AC_%EC%A0%84%EC%9E%90%EB%8F%84%EC%84%9C%EA%B4%80%20%EB%8F%84%EC%84%9C%EC%A0%95%EB%B3%B4_20250909"

# 3. ì„œì´ˆêµ¬ ë°ì´í„° ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ ìºì‹±)
@st.cache_data(ttl=86400, show_spinner=False)
def load_seocho_data():
    try:
        df = pd.read_csv(SEOCHO_CSV_URL, encoding='cp949')
        df.columns = df.columns.str.strip()
        for col in ['ë„ì„œëª…', 'ì €ìëª…', 'í˜•ì‹']:
            df[col] = df[col].astype(str).str.strip()
        return df[df['í˜•ì‹'].str.contains("ì „ìì±…", na=False)].copy()
    except:
        return None

df_seocho_cached = load_seocho_data()

# 4. ë„ì„œê´€ ëª©ë¡ ì •ì˜
libraries = [
    {"name": "ì„±ë‚¨ì‹œ", "url": "https://vodbook.snlib.go.kr/elibrary-front/search/searchList.ink", "key_param": "schTxt", "xpath": '//*[@id="container"]/div/div[4]/p/strong[2]/text()', "encoding": "utf-8", "type": "ink"},
    {"name": "ìš©ì¸ì‹œ", "url": "https://ebook.yongin.go.kr/elibrary-front/search/searchList.ink", "key_param": "schTxt", "xpath": '//*[@id="container"]/div/div[4]/p/strong[2]/text()', "encoding": "utf-8", "type": "ink"},
    {"name": "ìˆ˜ì›ì‹œ", "url": "https://ebook.suwonlib.go.kr/elibrary-front/search/searchList.ink", "key_param": "schTxt", "xpath": '//*[@id="container"]/div/div[4]/p/strong[2]/text()', "encoding": "utf-8", "type": "ink"},
    {"name": "ê³ ì–‘ì‹œ", "url": "https://ebook.goyanglib.or.kr/elibrary-front/search/searchList.ink", "key_param": "schTxt", "xpath": '//*[@id="container"]/div/div[4]/p/strong[2]/text()', "encoding": "utf-8", "type": "ink"},
    {"name": "ê²½ê¸°ëŒ€", "url": "https://ebook.kyonggi.ac.kr/elibrary-front/search/searchList.ink", "key_param": "schTxt", "xpath": '//*[@id="container"]/div/div[4]/p/strong[2]/text()', "encoding": "utf-8", "type": "ink"},
    {"name": "êµ¬ë…í˜•", "url": "https://lib.yongin.go.kr/intro/menu/10003/program/30012/plusSearchResultList.do", "key_param": "searchKeyword", "xpath": '//*[@id="searchForm"]/div/div[2]/div[1]/div[1]/strong[2]/text()', "encoding": "utf-8", "type": "subscription"},
    {"name": "ì„œìš¸ì‹œ", "url": "http://openapi.seoul.go.kr:8088/", "type": "seoul_api"},
    {"name": "ê°•ë‚¨êµ¬", "url": "https://ebook.gangnam.go.kr/elibbook/book_search_result.asp", "key_param": "sarg1", "xpath": '//*[@id="container"]/div[1]/div[2]/div[1]/div/div[2]/div[1]/div[1]/div/strong/text()', "encoding": "euc-kr", "type": "gangnam"},
    {"name": "ì„œì´ˆêµ¬", "type": "seocho_csv"},
]

def search_libraries(book_name):
    results = []
    progress_bar = st.progress(0)
    
    # ë³´ì•ˆ ì°¨ë‹¨ íšŒí”¼ìš© í—¤ë”
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    for i, lib in enumerate(libraries):
        progress_bar.progress((i + 1) / len(libraries))
        try:
            # A. ì„œì´ˆêµ¬ CSV ê²€ìƒ‰
            if lib["type"] == "seocho_csv":
                count = 0
                if df_seocho_cached is not None:
                    mask = (df_seocho_cached['ë„ì„œëª…'].str.contains(book_name, case=False, na=False)) | \
                           (df_seocho_cached['ì €ìëª…'].str.contains(book_name, case=False, na=False))
                    count = len(df_seocho_cached[mask].drop_duplicates(subset=['ë„ì„œëª…', 'ì €ìëª…', 'ì¶œíŒì‚¬']))
                results.append({"name": lib['name'], "link": f"https://e-book.seocholib.or.kr/search?keyword={quote(book_name)}", "status": f"{count}ê¶Œ" if count > 0 else "ì—†ìŒ"})

            # B. ì„œìš¸ë„ì„œê´€ API ê²€ìƒ‰ (ê³µë°± -> ì–¸ë”ë°”)
            elif lib["type"] == "seoul_api":
                if not SEOUL_API_KEY:
                    results.append({"name": lib['name'], "link": "#", "status": "í‚¤ ì„¤ì • í•„ìš”"})
                    continue
                unique_books = {}
                processed_name = book_name.replace(" ", "_")
                encoded_kw = quote(processed_name)
                for path in [f"1/500/{encoded_kw}/%20/%20/%20/%20", f"1/500/%20/{encoded_kw}/%20/%20/%20"]:
                    resp = requests.get(f"{lib['url']}{SEOUL_API_KEY}/json/SeoulLibraryBookSearchInfo/{path}", timeout=10)
                    if resp.status_code == 200:
                        data = resp.json()
                        if "SeoulLibraryBookSearchInfo" in data:
                            for book in data["SeoulLibraryBookSearchInfo"].get("row", []):
                                if book.get("BIB_TYPE_NAME") == "ì „ìì±…":
                                    unique_books[book.get("CTRLNO")] = book
                count = len(unique_books)
                results.append({"name": lib['name'], "link": f"https://elib.seoul.go.kr/contents/search/content?t=EB&k={quote(book_name)}", "status": f"{count}ê¶Œ" if count > 0 else "ì—†ìŒ"})

            # C. ìŠ¤í¬ë˜í•‘ ë°©ì‹ (ìš©ì¸ êµ¬ë…í˜• í¬í•¨)
            else:
                encoded_query = quote(book_name.encode(lib["encoding"]))
                if lib["type"] == "subscription":
                    search_url = f"{lib['url']}?searchType=SIMPLE&searchCategory=EBOOK2&searchKey=ALL&searchKeyword={encoded_query}"
                elif lib["type"] == "gangnam":
                    search_url = f"{lib['url']}?scon1=TITLE&sarg1={encoded_query}&sopr2=OR&scon2=AUTHOR&sarg2={encoded_query}"
                else:
                    search_url = f"{lib['url']}?{lib['key_param']}={encoded_query}&schClst=ctts%2Cautr&schDvsn=001"
                
                # í—¤ë” ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì²­ (ì°¨ë‹¨ ë°©ì§€)
                resp = requests.get(search_url, timeout=10, headers=headers)
                tree = html.fromstring(resp.content)
                nodes = tree.xpath(lib["xpath"])
                
                # ì •ê·œì‹ì„ ì´ìš©í•´ í…ìŠ¤íŠ¸ ë‚´ ìˆ«ìë§Œ ì¶”ì¶œ
                raw_text = "".join(nodes).strip()
                count_match = re.findall(r'\d+', raw_text)
                count = int(count_match[0]) if count_match else 0
                results.append({"name": lib['name'], "link": search_url, "status": f"{count}ê¶Œ" if count > 0 else "ì—†ìŒ"})
        except:
            results.append({"name": lib['name'], "link": "#", "status": "í™•ì¸ë¶ˆê°€"})

    progress_bar.empty()
    return results

# --- ë©”ì¸ í™”ë©´ UI ---
st.markdown('<h2 style="font-size:24px; margin-top:-50px;">ğŸ“š ì „ìë„ì„œê´€ í†µí•©ê²€ìƒ‰</h2>', unsafe_allow_html=True)
keyword = st.text_input("ì±… ì œëª© ë˜ëŠ” ì €ìë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ë…¸ì¸ê³¼ ë°”ë‹¤")

if keyword:
    with st.spinner(f"ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
        data = search_libraries(keyword)
        
        # í…Œì´ë¸” ìƒì„±
        html_code = """<div style="font-family:sans-serif;"><table style="width:100%; border-collapse:collapse;">
        <thead><tr style="background:#f8f9fa; border-bottom:2px solid #ddd;"><th style="text-align:left; padding:12px;">ë„ì„œê´€</th><th style="text-align:right; padding:12px;">í˜„í™©</th></tr></thead><tbody>"""
        for item in data:
            html_code += f"""<tr style="border-bottom:1px solid #eee;"><td style="padding:12px; font-weight:bold;">{item['name']}</td>
            <td style="padding:12px; text-align:right;"><a href="{item['link']}" target="_blank" style="color:#007bff; text-decoration:none; font-weight:bold;">{item['status']}</a></td></tr>"""
        
        st.components.v1.html(html_code + "</tbody></table></div>", height=len(data) * 52 + 60)
        
        # í…Œì´ë¸” í•˜ë‹¨ ì„œì´ˆêµ¬ ê³ ì • ë©”ì‹œì§€
        st.markdown("---")
        st.info("ğŸ“¢ ì„œì´ˆêµ¬ ë°ì´í„° ì—…ë°ì´íŠ¸ ì˜ˆì •ì¼ : 2026.3.4")
